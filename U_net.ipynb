{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a498723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers, models, mixed_precision\n",
    "from PIL import Image\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import time # 用于计时\n",
    "\n",
    "# --- 初始配置与全局参数 ---\n",
    "print(\"--- 初始化 TensorFlow 和环境配置 ---\")\n",
    "print(\"TensorFlow 版本:\", tf.__version__)\n",
    "\n",
    "# 1. 配置 GPU\n",
    "print(\"\\n--- 配置 GPU ---\")\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if len(physical_devices) > 0:\n",
    "    try:\n",
    "        # 设置内存增长，避免一次性占用所有显存\n",
    "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "        print(f\"GPU 可用，内存增长已启用: {physical_devices[0].name}\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"无法设置内存增长: {e}\")\n",
    "else:\n",
    "    print(\"未检测到 GPU，将使用 CPU (速度会非常慢)\")\n",
    "print(\"--- GPU 配置结束 ---\")\n",
    "\n",
    "# 2. 配置混合精度\n",
    "print(\"\\n--- 配置混合精度 ---\")\n",
    "policy_name = 'mixed_float16' # 适用于 Compute Capability >= 7.0 的 NVIDIA GPU\n",
    "gpu_supported_fp16 = False\n",
    "if physical_devices:\n",
    "    try:\n",
    "        details = tf.config.experimental.get_device_details(physical_devices[0])\n",
    "        cc = details.get('compute_capability')\n",
    "        if cc and cc[0] >= 7:\n",
    "            gpu_supported_fp16 = True\n",
    "            print(f\"检测到支持 FP16 的 GPU (Compute Capability {cc[0]}.{cc[1]})\")\n",
    "        else:\n",
    "            print(f\"GPU Compute Capability ({cc}) 可能不支持 FP16 或效率不高 (需要 >= 7.0)\")\n",
    "    except Exception as e:\n",
    "         print(f\"检查 GPU 能力时出错: {e}，假定不支持 FP16\")\n",
    "\n",
    "if gpu_supported_fp16:\n",
    "    try:\n",
    "        print(f\"尝试启用混合精度策略 '{policy_name}'\")\n",
    "        policy = mixed_precision.Policy(policy_name)\n",
    "        mixed_precision.set_global_policy(policy)\n",
    "        print('策略设置成功!')\n",
    "        print(f'计算精度 (Compute dtype): {policy.compute_dtype}')\n",
    "        print(f'变量精度 (Variable dtype): {policy.variable_dtype}')\n",
    "    except Exception as e:\n",
    "        print(f\"设置混合精度策略 '{policy_name}' 时出错: {e}。将使用 float32。\")\n",
    "        policy = mixed_precision.Policy('float32')\n",
    "        mixed_precision.set_global_policy(policy)\n",
    "        print(\"已回退到 float32 策略。\")\n",
    "else:\n",
    "    print(\"未检测到支持 FP16 的 GPU 或不满足条件。使用默认 float32 精度。\")\n",
    "    policy = mixed_precision.Policy('float32') # 确保 policy 被定义\n",
    "    mixed_precision.set_global_policy(policy)\n",
    "    print('计算精度 (Compute dtype): float32')\n",
    "    print('变量精度 (Variable dtype): float32')\n",
    "print(\"--- 混合精度配置结束 ---\")\n",
    "\n",
    "# 3. 路径设置\n",
    "base_path = r\"D:\\Class\\CV\\Task2\\CDSet\\dataset_YOLO_format_3434\" # *请根据你的实际路径修改*\n",
    "train_img_path = os.path.join(base_path, \"images\", \"train\")\n",
    "train_mask_path = os.path.join(base_path, \"masks_crosswalk\", \"train\") # 使用预处理的掩码\n",
    "test_img_path = os.path.join(base_path, \"images\", \"test\")\n",
    "test_mask_path = os.path.join(base_path, \"masks_crosswalk\", \"test\")   # 使用预处理的掩码\n",
    "\n",
    "# 4. 图像和训练参数\n",
    "IMG_HEIGHT = 352  # 降低后的分辨率\n",
    "IMG_WIDTH = 640   # 降低后的分辨率\n",
    "NUM_CLASSES = 2   # 背景 (0) + crosswalk (1)\n",
    "BATCH_SIZE = 8    # 增大后的批量大小 (如果OOM则减小)\n",
    "EPOCHS = 5        # 训练轮数\n",
    "\n",
    "print(\"\\n--- 全局参数设置 ---\")\n",
    "print(f\"图像分辨率: {IMG_HEIGHT}x{IMG_WIDTH}\")\n",
    "print(f\"批量大小 (Batch Size): {BATCH_SIZE}\")\n",
    "print(f\"目标类别数 (含背景): {NUM_CLASSES}\")\n",
    "print(f\"训练轮数 (Epochs): {EPOCHS}\")\n",
    "print(f\"训练图像路径: {train_img_path}\")\n",
    "print(f\"训练掩码路径: {train_mask_path}\")\n",
    "print(f\"测试图像路径: {test_img_path}\")\n",
    "print(f\"测试掩码路径: {test_mask_path}\")\n",
    "print(\"--- 参数设置结束 ---\")\n",
    "\n",
    "# --- 辅助函数定义 ---\n",
    "\n",
    "# 5. 数据生成器 (加载和基础预处理)\n",
    "def create_dataset(img_dir, mask_dir, batch_size, shuffle=True, is_training=True):\n",
    "    \"\"\"\n",
    "    创建 TensorFlow 数据集。\n",
    "    加载图像和对应的单通道掩码，进行缩放和归一化。\n",
    "    \"\"\"\n",
    "    phase = \"训练\" if is_training else \"测试\"\n",
    "    print(f\"\\n--- 开始准备 {phase} 数据集 ---\")\n",
    "    print(f\"正在从 {img_dir} 和 {mask_dir} 匹配图像和掩码...\")\n",
    "    img_files = glob.glob(os.path.join(img_dir, \"*.jpg\")) + glob.glob(os.path.join(img_dir, \"*.png\"))\n",
    "\n",
    "    mask_files = []\n",
    "    valid_img_files = []\n",
    "    for img_f in img_files:\n",
    "        base_name = os.path.splitext(os.path.basename(img_f))[0]\n",
    "        mask_f = os.path.join(mask_dir, base_name + \".png\") # 查找对应的 .png 掩码\n",
    "        if os.path.exists(mask_f):\n",
    "            mask_files.append(mask_f)\n",
    "            valid_img_files.append(img_f)\n",
    "        # else:\n",
    "        #     # print(f\"警告：找不到图像 '{img_f}' 对应的掩码文件 '{mask_f}'\") # 取消过多打印\n",
    "        #     pass\n",
    "\n",
    "    print(f\"找到 {len(img_files)} 张图像，{len(valid_img_files)} 个有效图像-掩码对\")\n",
    "\n",
    "    if not valid_img_files:\n",
    "        raise ValueError(f\"在 {mask_dir} 中没有找到有效的掩码文件。请确保已运行预处理脚本！\")\n",
    "\n",
    "    # 定义加载和基础预处理函数 (不含 one-hot)\n",
    "    def load_and_preprocess(img_path, mask_path):\n",
    "        img = tf.io.read_file(img_path)\n",
    "        img = tf.image.decode_image(img, channels=3, expand_animations=False)\n",
    "        img = tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])\n",
    "        img = tf.cast(img, tf.float32) / 255.0\n",
    "        img = tf.clip_by_value(img, 0.0, 1.0)\n",
    "        img = tf.ensure_shape(img, [IMG_HEIGHT, IMG_WIDTH, 3])\n",
    "\n",
    "        mask = tf.io.read_file(mask_path)\n",
    "        mask = tf.image.decode_png(mask, channels=1)\n",
    "        mask = tf.image.resize(mask, [IMG_HEIGHT, IMG_WIDTH], method='nearest')\n",
    "        mask = tf.cast(mask, tf.uint8)\n",
    "        mask = tf.clip_by_value(mask, 0, NUM_CLASSES - 1)\n",
    "        mask = tf.ensure_shape(mask, [IMG_HEIGHT, IMG_WIDTH, 1])\n",
    "        return img, mask\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((valid_img_files, mask_files))\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(valid_img_files))\n",
    "\n",
    "    dataset = dataset.map(load_and_preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(tf.data.AUTOTUNE) # 提升性能\n",
    "\n",
    "    print(f\"--- {phase} 数据集准备完成 ---\")\n",
    "    return dataset\n",
    "\n",
    "# 6. 数据可视化函数 (可视化原始单通道掩码)\n",
    "def visualize_dataset(dataset, num_samples=3):\n",
    "    \"\"\"\n",
    "    从数据集中取样并可视化图像和对应的单通道掩码。\n",
    "    \"\"\"\n",
    "    print(\"\\n--- 可视化部分数据样本 (图像和对应原始掩码) ---\")\n",
    "    try:\n",
    "        for img_batch, mask_batch in dataset.take(1): # 只取一个批次\n",
    "            for i in range(min(num_samples, img_batch.shape[0])):\n",
    "                plt.figure(figsize=(12, 4))\n",
    "                plt.subplot(1, 2, 1)\n",
    "                plt.title(\"Image\")\n",
    "                plt.imshow(img_batch[i])\n",
    "                plt.axis('off')\n",
    "\n",
    "                plt.subplot(1, 2, 2)\n",
    "                plt.title(\"Mask (Crosswalk=1, Background=0)\")\n",
    "                # 显示 uint8 的单通道掩码\n",
    "                plt.imshow(tf.squeeze(mask_batch[i]), cmap='gray', vmin=0, vmax=NUM_CLASSES-1)\n",
    "                plt.colorbar()\n",
    "                plt.axis('off')\n",
    "                plt.show()\n",
    "            break # 只显示第一批次的前几个\n",
    "    except Exception as e:\n",
    "        print(f\"可视化数据时出错: {e}\")\n",
    "        print(\"请确保数据集已正确加载。\")\n",
    "    print(\"--- 可视化结束 ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bbee73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. One-Hot 编码预处理函数 (用于训练)\n",
    "@tf.function # 转换为 TF 图以提高效率\n",
    "def preprocess_data_for_training(img, mask):\n",
    "    \"\"\"\n",
    "    将单通道掩码转换为 One-Hot 编码，用于训练。\n",
    "    \"\"\"\n",
    "    mask = tf.cast(mask, tf.int32)\n",
    "    mask = tf.squeeze(mask, axis=-1) # (H, W, 1) -> (H, W)\n",
    "    mask = tf.one_hot(mask, depth=NUM_CLASSES, dtype=tf.float32) # (H, W) -> (H, W, NUM_CLASSES)\n",
    "    return img, mask\n",
    "\n",
    "# 8. U-Net 模型定义\n",
    "def unet_model(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), num_classes=NUM_CLASSES):\n",
    "    \"\"\"\n",
    "    定义 U-Net 模型结构。\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "    # 编码器 (下采样)\n",
    "    c1 = layers.Conv2D(32, 3, activation='relu', padding='same', kernel_initializer='he_normal')(inputs)\n",
    "    c1 = layers.Conv2D(32, 3, activation='relu', padding='same', kernel_initializer='he_normal')(c1)\n",
    "    p1 = layers.MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = layers.Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(p1)\n",
    "    c2 = layers.Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(c2)\n",
    "    p2 = layers.MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    c3 = layers.Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(p2)\n",
    "    c3 = layers.Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(c3)\n",
    "    p3 = layers.MaxPooling2D((2, 2))(c3)\n",
    "\n",
    "    c4 = layers.Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(p3)\n",
    "    c4 = layers.Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(c4)\n",
    "    p4 = layers.MaxPooling2D((2, 2))(c4)\n",
    "\n",
    "    # 瓶颈层\n",
    "    c5 = layers.Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(p4)\n",
    "    c5 = layers.Conv2D(512, 3, activation='relu', padding='same', kernel_initializer='he_normal')(c5)\n",
    "\n",
    "    # 解码器 (上采样 + 跳跃连接)\n",
    "    u6 = layers.Conv2DTranspose(256, 2, strides=(2, 2), padding='same')(c5)\n",
    "    u6 = layers.concatenate([u6, c4])\n",
    "    c6 = layers.Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(u6)\n",
    "    c6 = layers.Conv2D(256, 3, activation='relu', padding='same', kernel_initializer='he_normal')(c6)\n",
    "\n",
    "    u7 = layers.Conv2DTranspose(128, 2, strides=(2, 2), padding='same')(c6)\n",
    "    u7 = layers.concatenate([u7, c3])\n",
    "    c7 = layers.Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(u7)\n",
    "    c7 = layers.Conv2D(128, 3, activation='relu', padding='same', kernel_initializer='he_normal')(c7)\n",
    "\n",
    "    u8 = layers.Conv2DTranspose(64, 2, strides=(2, 2), padding='same')(c7)\n",
    "    u8 = layers.concatenate([u8, c2])\n",
    "    c8 = layers.Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(u8)\n",
    "    c8 = layers.Conv2D(64, 3, activation='relu', padding='same', kernel_initializer='he_normal')(c8)\n",
    "\n",
    "    u9 = layers.Conv2DTranspose(32, 2, strides=(2, 2), padding='same')(c8)\n",
    "    u9 = layers.concatenate([u9, c1])\n",
    "    c9 = layers.Conv2D(32, 3, activation='relu', padding='same', kernel_initializer='he_normal')(u9)\n",
    "    c9 = layers.Conv2D(32, 3, activation='relu', padding='same', kernel_initializer='he_normal')(c9)\n",
    "\n",
    "    # 输出层 - 使用 softmax 输出概率\n",
    "    outputs = layers.Conv2D(num_classes, 1, activation='softmax')(c9)\n",
    "\n",
    "    # 重要: 确保混合精度训练时最后输出是 float32\n",
    "    outputs = layers.Activation('linear', dtype='float32')(outputs)\n",
    "\n",
    "    model = models.Model(inputs, outputs, name=\"unet_crosswalk\")\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211bb22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. 自定义回调函数 (用于监控训练)\n",
    "class CustomCallback(tf.keras.callbacks.Callback):\n",
    "    \"\"\"\n",
    "    自定义回调，用于在每个 Epoch 结束时打印耗时和指标。\n",
    "    \"\"\"\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch_start_time = time.time()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        epoch_duration = time.time() - self.epoch_start_time\n",
    "        print(f\"\\nEpoch {epoch + 1}/{self.params['epochs']} 完成，耗时: {epoch_duration:.2f} 秒\")\n",
    "        if logs: # 确保 logs 不为 None\n",
    "            print(f\"  训练集 - loss: {logs.get('loss'):.4f}, accuracy: {logs.get('accuracy'):.4f}, mean_io_u: {logs.get('mean_io_u'):.4f}\")\n",
    "            print(f\"  验证集 - val_loss: {logs.get('val_loss'):.4f}, val_accuracy: {logs.get('val_accuracy'):.4f}, val_mean_io_u: {logs.get('val_mean_io_u'):.4f}\")\n",
    "        else:\n",
    "            print(\"  未能获取此 Epoch 的训练指标。\")\n",
    "\n",
    "# 10. 训练主函数\n",
    "def train_model():\n",
    "    \"\"\"\n",
    "    封装了模型训练的主要流程：数据准备、模型构建、编译和训练。\n",
    "    \"\"\"\n",
    "    # 创建基础数据集 (不含 one-hot)\n",
    "    train_dataset_base = create_dataset(train_img_path, train_mask_path, BATCH_SIZE, shuffle=True, is_training=True)\n",
    "    test_dataset_base = create_dataset(test_img_path, test_mask_path, BATCH_SIZE, shuffle=False, is_training=False)\n",
    "\n",
    "    # 可视化基础数据集中的样本\n",
    "    visualize_dataset(train_dataset_base, num_samples=2)\n",
    "\n",
    "    # 应用 One-Hot 编码，得到用于训练的数据集\n",
    "    print(\"\\n--- 对数据集应用 One-Hot 编码 ---\")\n",
    "    train_dataset = train_dataset_base.map(preprocess_data_for_training, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    test_dataset = test_dataset_base.map(preprocess_data_for_training, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    print(\"--- One-Hot 编码应用完成 ---\")\n",
    "\n",
    "    # 检查一个批次的形状 (调试)\n",
    "    print(\"\\n--- 检查训练数据批次形状 ---\")\n",
    "    for img_batch, mask_batch in train_dataset.take(1):\n",
    "        print(\"应用 preprocess_data_for_training 后:\")\n",
    "        print(f\"图像批次形状: {img_batch.shape}\") # 应为 (B, H, W, 3)\n",
    "        print(f\"掩码批次形状: {mask_batch.shape}\") # 应为 (B, H, W, NUM_CLASSES)\n",
    "        print(f\"掩码数据类型: {mask_batch.dtype}\") # 应为 float32\n",
    "        # 检查 One-Hot\n",
    "        sample_pixel = mask_batch[0, IMG_HEIGHT // 2, IMG_WIDTH // 2, :]\n",
    "        print(f\"掩码 One-Hot 检查 (样本0中心像素): {sample_pixel.numpy()}\")\n",
    "        print(f\"掩码 One-Hot 检查 (中心像素类别和): {tf.reduce_sum(sample_pixel).numpy()}\") # 应为 1.0\n",
    "    print(\"--- 形状检查结束 ---\")\n",
    "\n",
    "    print(\"\\n--- 构建和编译模型 ---\")\n",
    "    model = unet_model(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), num_classes=NUM_CLASSES)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy', # 因为使用了 one-hot\n",
    "                  metrics=['accuracy', tf.keras.metrics.MeanIoU(num_classes=NUM_CLASSES, name='mean_io_u')])\n",
    "    model.summary()\n",
    "    print(\"--- 模型构建和编译完成 ---\")\n",
    "\n",
    "    print(f\"\\n--- 开始训练，共 {EPOCHS} 个 Epoch ---\")\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=test_dataset,\n",
    "        callbacks=[CustomCallback()]\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    total_training_time = end_time - start_time\n",
    "    print(f\"--- 训练完成，总耗时: {total_training_time:.2f} 秒 ---\")\n",
    "\n",
    "    # 保存模型\n",
    "    model_save_path = \"unet_crosswalk_model_final_refactored.h5\"\n",
    "    print(f\"\\n--- 保存模型到 {model_save_path} ---\")\n",
    "    try:\n",
    "        model.save(model_save_path)\n",
    "        print(\"--- 模型保存完成 ---\")\n",
    "    except Exception as e:\n",
    "        print(f\"模型保存失败: {e}\")\n",
    "\n",
    "    return history, model\n",
    "\n",
    "# 11. 绘制训练曲线函数\n",
    "def plot_history(history):\n",
    "    \"\"\"\n",
    "    绘制训练过程中的准确率、损失和 MeanIoU 曲线。\n",
    "    \"\"\"\n",
    "    print(\"\\n--- 绘制训练和验证曲线 ---\")\n",
    "    plt.figure(figsize=(18, 5))\n",
    "\n",
    "    # 准确率\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(history.history.get('accuracy', []), label='Train Accuracy', marker='o')\n",
    "    plt.plot(history.history.get('val_accuracy', []), label='Val Accuracy', marker='x')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # 损失\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(history.history.get('loss', []), label='Train Loss', marker='o')\n",
    "    plt.plot(history.history.get('val_loss', []), label='Val Loss', marker='x')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # MeanIoU\n",
    "    iou_metric_name = 'mean_io_u'\n",
    "    val_iou_metric_name = 'val_mean_io_u'\n",
    "    if iou_metric_name in history.history and val_iou_metric_name in history.history:\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(history.history[iou_metric_name], label='Train MeanIoU', marker='o')\n",
    "        plt.plot(history.history[val_iou_metric_name], label='Val MeanIoU', marker='x')\n",
    "        plt.title('Model Mean IoU')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Mean IoU')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "    else:\n",
    "        print(f\"警告：找不到 MeanIoU 指标 ('{iou_metric_name}' 或 '{val_iou_metric_name}')\")\n",
    "        print(\"可用指标:\", list(history.history.keys()))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('U-Net Training Metrics', fontsize=16, y=1.02)\n",
    "    plt.show()\n",
    "    print(\"--- 曲线绘制完成 ---\")\n",
    "\n",
    "# 12. 测试和可视化预测结果函数\n",
    "def test_and_visualize(model, num_samples=1):\n",
    "    \"\"\"\n",
    "    在测试集上进行预测，并可视化输入图像、真实掩码和预测掩码。\n",
    "    \"\"\"\n",
    "    print(\"\\n--- 开始在测试集上进行预测和可视化 ---\")\n",
    "    # 需要重新创建一个不带 prefetch 的数据集，或者一个只用于可视化的数据集\n",
    "    # 这里我们复用 create_dataset，但不进行 shuffle 和 prefetch (prefetch 可能影响 take)\n",
    "    test_dataset_for_vis = create_dataset(test_img_path, test_mask_path, batch_size=BATCH_SIZE, shuffle=False, is_training=False)\n",
    "    test_dataset_for_vis = test_dataset_for_vis.unbatch().batch(BATCH_SIZE) # 确保批次大小一致，但不预取\n",
    "\n",
    "    samples_shown = 0\n",
    "    try:\n",
    "        for img_batch, mask_batch_true_label in test_dataset_for_vis: # mask 是 (B, H, W, 1), 值为 0 或 1\n",
    "            # 模型预测\n",
    "            predictions_probs = model.predict(img_batch) # (B, H, W, 2) 概率\n",
    "            predictions_labels = tf.argmax(predictions_probs, axis=-1) # (B, H, W) 标签\n",
    "\n",
    "            for i in range(img_batch.shape[0]):\n",
    "                if samples_shown >= num_samples:\n",
    "                    break\n",
    "\n",
    "                plt.figure(figsize=(18, 6))\n",
    "\n",
    "                # 原图\n",
    "                plt.subplot(1, 3, 1)\n",
    "                plt.title(\"Input Image\")\n",
    "                plt.imshow(img_batch[i])\n",
    "                plt.axis('off')\n",
    "\n",
    "                # 真实掩码 (Ground Truth)\n",
    "                plt.subplot(1, 3, 2)\n",
    "                plt.title(\"Ground Truth (Crosswalk=1)\")\n",
    "                plt.imshow(tf.squeeze(mask_batch_true_label[i]), cmap='gray', vmin=0, vmax=1) # 显示原始 0/1 掩码\n",
    "                plt.axis('off')\n",
    "\n",
    "                # 预测掩码\n",
    "                plt.subplot(1, 3, 3)\n",
    "                plt.title(\"Prediction (Crosswalk=1)\")\n",
    "                plt.imshow(predictions_labels[i], cmap='gray', vmin=0, vmax=1) # 显示预测 0/1 标签\n",
    "                plt.axis('off')\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                samples_shown += 1\n",
    "\n",
    "            if samples_shown >= num_samples:\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(f\"测试和可视化过程中出错: {e}\")\n",
    "    print(\"--- 预测可视化完成 ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab037047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 主执行流程 ---\n",
    "if __name__ == \"__main__\": # 适用于脚本执行，在 Notebook 中直接运行即可\n",
    "    # 训练模型\n",
    "    history, trained_model = train_model()\n",
    "\n",
    "    # 绘制历史曲线\n",
    "    if history:\n",
    "        plot_history(history)\n",
    "    else:\n",
    "        print(\"未能获取训练历史，无法绘制曲线。\")\n",
    "\n",
    "    # 在测试集上可视化结果\n",
    "    if trained_model:\n",
    "        test_and_visualize(trained_model, num_samples=3) # 可视化 3 个样本\n",
    "    else:\n",
    "        print(\"模型训练失败或未返回，无法进行测试可视化。\")\n",
    "\n",
    "    print(\"\\n--- 脚本执行完毕 ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826a1a73",
   "metadata": {},
   "source": [
    "# 应用到自己的图像 （要提前处理成352*640适应模型）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e962e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, mixed_precision\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "\n",
    "IMG_HEIGHT = 352\n",
    "IMG_WIDTH = 640\n",
    "NUM_CLASSES = 2\n",
    "MODEL_PATH = \"unet_crosswalk_model_final.h5\" \n",
    "\n",
    "INPUT_IMAGE_DIR = r\"D:\\Class\\CV\\Task2\\CampusZebra_new\" \n",
    "OUTPUT_MASK_DIR = r\"D:\\Class\\CV\\Task2\\CampusZebra_result\" \n",
    "#可视化数量\n",
    "NUM_SAMPLES_TO_SHOW = 2\n",
    "\n",
    "def preprocess_single_image_for_prediction(image_path, target_height=IMG_HEIGHT, target_width=IMG_WIDTH):\n",
    "\n",
    "    try:\n",
    "        img = Image.open(image_path).convert('RGB') \n",
    "        img_resized = img.resize((target_width, target_height), Image.Resampling.LANCZOS)\n",
    "        img_array = np.array(img_resized)\n",
    "        img_normalized = img_array.astype(np.float32) / 255.0\n",
    "        img_normalized = np.clip(img_normalized, 0.0, 1.0)\n",
    "        img_batch = np.expand_dims(img_normalized, axis=0)\n",
    "        return img_batch\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def predict_and_save_masks(model, input_dir, output_dir):\n",
    "    print(f\"\\n--- 开始批量预测 (可视化前) ---\")\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    image_files = glob.glob(os.path.join(input_dir, \"*.jpg\")) + glob.glob(os.path.join(input_dir, \"*.png\"))\n",
    "    if not image_files:\n",
    "        print(f\"错误：在 {input_dir} 未找到图像。\")\n",
    "        return\n",
    "\n",
    "    print(f\"找到 {len(image_files)} 张图像进行预测...\")\n",
    "    for i, img_path in enumerate(image_files):\n",
    "        img_batch = preprocess_single_image_for_prediction(img_path)\n",
    "        if img_batch is None: continue\n",
    "\n",
    "        predictions_probs = model.predict(img_batch, verbose=0)\n",
    "        predictions_labels = tf.argmax(predictions_probs, axis=-1)\n",
    "        predicted_mask_labels = tf.squeeze(predictions_labels, axis=0).numpy()\n",
    "        output_mask_array = (predicted_mask_labels * 255).astype(np.uint8)\n",
    "        output_mask_image = Image.fromarray(output_mask_array, mode='L')\n",
    "\n",
    "        base_name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "        output_filename = base_name + \"_pred_mask.png\"\n",
    "        output_path = os.path.join(output_dir, output_filename)\n",
    "        try:\n",
    "            output_mask_image.save(output_path)\n",
    "        except Exception as e:\n",
    "            print(f\"保存掩码 {output_filename} 出错: {e}\")\n",
    "    print(f\"--- 批量预测完成，掩码保存在: {output_dir} ---\")\n",
    "\n",
    "\n",
    "# ---可视化函数 ---\n",
    "def visualize_predictions(input_dir, predicted_mask_dir, num_samples=NUM_SAMPLES_TO_SHOW):\n",
    "    \"\"\"\n",
    "    加载原始图像和对应的已保存预测掩码，并排显示。\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- 开始可视化预测结果 ---\")\n",
    "    print(f\"原始图像目录: {input_dir}\")\n",
    "    print(f\"预测掩码目录: {predicted_mask_dir}\")\n",
    "\n",
    "    if not os.path.isdir(input_dir):\n",
    "        print(f\"错误：找不到输入图像目录 {input_dir}\")\n",
    "        return\n",
    "    if not os.path.isdir(predicted_mask_dir):\n",
    "        print(f\"错误：找不到预测掩码目录 {predicted_mask_dir}。请先运行预测。\")\n",
    "        return\n",
    "\n",
    "    image_files = glob.glob(os.path.join(input_dir, \"*.jpg\")) + glob.glob(os.path.join(input_dir, \"*.png\"))\n",
    "\n",
    "    if not image_files:\n",
    "        print(f\"错误：在 {input_dir} 中未找到用于可视化的图像。\")\n",
    "        return\n",
    "\n",
    "    print(f\"将尝试可视化最多 {num_samples} 个样本...\")\n",
    "    samples_shown = 0\n",
    "    for img_path in image_files:\n",
    "        if samples_shown >= num_samples:\n",
    "            break\n",
    "\n",
    "        base_name = os.path.splitext(os.path.basename(img_path))[0]\n",
    "        predicted_mask_filename = base_name + \"_pred_mask.png\"\n",
    "        predicted_mask_path = os.path.join(predicted_mask_dir, predicted_mask_filename)\n",
    "\n",
    "        if not os.path.exists(predicted_mask_path):\n",
    "            print(f\"警告：找不到图像 '{os.path.basename(img_path)}' 对应的预测掩码 '{predicted_mask_filename}'，跳过。\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # 加载原始图像\n",
    "            original_image = Image.open(img_path).convert('RGB')\n",
    "            # 加载预测掩码 (灰度图)\n",
    "            predicted_mask_image = Image.open(predicted_mask_path).convert('L')\n",
    "\n",
    "            # 创建图像\n",
    "            plt.figure(figsize=(12, 6))\n",
    "\n",
    "            # 左侧：原始图像\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.title(f\"Original Image: {os.path.basename(img_path)}\")\n",
    "            plt.imshow(original_image)\n",
    "            plt.axis('off')\n",
    "\n",
    "            # 右侧：预测掩码\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.title(f\"Predicted Mask: {predicted_mask_filename}\")\n",
    "            plt.imshow(predicted_mask_image, cmap='gray', vmin=0, vmax=255) # 灰度图显示\n",
    "            plt.axis('off')\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            samples_shown += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"处理或显示图像 {os.path.basename(img_path)} 时出错: {e}\")\n",
    "\n",
    "    if samples_shown == 0:\n",
    "         print(\"未能成功可视化任何样本。请检查路径和文件是否存在。\")\n",
    "    print(f\"--- 可视化完成，共显示 {samples_shown} 个样本 ---\")\n",
    "\n",
    "\n",
    "# --- 主执行部分 ---\n",
    "if __name__ == \"__main__\":\n",
    "    # 检查输入输出目录是否已设置\n",
    "    if \"path/to/your\" in INPUT_IMAGE_DIR or \"path/to/save\" in OUTPUT_MASK_DIR:\n",
    "         print(\"错误：请先在代码中修改 INPUT_IMAGE_DIR 和 OUTPUT_MASK_DIR 为你的实际路径！\")\n",
    "    else:\n",
    "        # ---- 选择性执行：如果还没有预测结果，先执行预测 ----\n",
    "        # 1. 加载模型\n",
    "        if not os.path.exists(MODEL_PATH):\n",
    "            print(f\"错误：找不到模型文件 {MODEL_PATH}。无法执行预测。\")\n",
    "            model_available = False\n",
    "        else:\n",
    "            try:\n",
    "                loaded_model = models.load_model(MODEL_PATH, compile=False)\n",
    "                print(\"模型加载成功！\")\n",
    "                model_available = True\n",
    "            except Exception as e:\n",
    "                print(f\"加载模型时出错: {e}\")\n",
    "                model_available = False\n",
    "\n",
    "        # 2. 运行预测 (如果需要且模型可用)\n",
    "        run_prediction = False # 设置为 True 如果你需要先生成预测掩码文件\n",
    "        if run_prediction and model_available:\n",
    "             predict_and_save_masks(loaded_model, INPUT_IMAGE_DIR, OUTPUT_MASK_DIR)\n",
    "        elif run_prediction and not model_available:\n",
    "             print(\"模型未加载，无法执行预测。\")\n",
    "        else:\n",
    "             print(\"跳过预测步骤，直接进行可视化（假设掩码已存在）。\")\n",
    "        # ---- 预测部分结束 ----\n",
    "\n",
    "\n",
    "        # 3. 可视化结果 (无论是否刚刚预测，都尝试可视化)\n",
    "        visualize_predictions(INPUT_IMAGE_DIR, OUTPUT_MASK_DIR, num_samples=NUM_SAMPLES_TO_SHOW)\n",
    "\n",
    "    print(\"\\n--- 脚本执行完毕 ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
